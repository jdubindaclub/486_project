{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from scipy.stats import zscore\n",
    "import os, pickle, time\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#local imports\n",
    "from deeplearning.util import *\n",
    "from deeplearning import loss_fxs, transforms\n",
    "from deeplearning import callbacks\n",
    "import deeplearning.metrics as metrics_api\n",
    "from deeplearning import datasets\n",
    "from deeplearning import fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "device = \"mps\" if getattr(torch,'has_mps',False) \\\n",
    "    else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"smoking_drinking_hyperparams.csv\")\n",
    "df['DRK_YN'] = df['DRK_YN'].map({'Y': 1, 'N': 0})\n",
    "df['sex'] = df['sex'].map({'Male': 1, 'Female': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_args = {\n",
    "    'emb_szs': [],\n",
    "    'out_sz': 2,\n",
    "    'layers': [3000, 850, 120],\n",
    "    'ps': [0.2, 0.2, 0.05],\n",
    "    'sigma': 0.8,\n",
    "}\n",
    "n_iter = 0\n",
    "max_lr = 1e-5,\n",
    "n_epochs = 2\n",
    "pct_start = 0.3\n",
    "grad_clip = 1.0\n",
    "num_workers = 0\n",
    "lams = 0.05\n",
    "cb_metrics = [metrics_api.AUC(), metrics_api.Last()]\n",
    "train_dl_shuffle = True\n",
    "train_batch_size = 128\n",
    "val_batch_size = 128\n",
    "train_df_pre, val_df_pre = train_test_split(df, test_size=0.2, random_state=42)\n",
    "opt_kw = {'weight_decay': 0.01}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cols = df.columns.tolist()\n",
    "x_cols.remove('DRK_YN')\n",
    "# cat_cols = ['sex']\n",
    "# cont_cols = x_cols.copy()\n",
    "# cont_cols.remove('sex')\n",
    "y_cols = ['DRK_YN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = 'deeplearning/models/newmodel'\n",
    "cwd = os.getcwd()\n",
    "save_dir = os.path.join(cwd, save_dir)\n",
    "# os.makedirs(save_dir, exist_ok=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_kwargs = {}\n",
    "lr_kwargs['max_lr'] = max_lr\n",
    "lr_kwargs['n_epochs'] = n_epochs\n",
    "lr_kwargs['pct_start'] = pct_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_sched_factory(opt, max_lr=None, n_epochs=None, steps_per_epoch=None, **kwargs):\n",
    "    return torch.optim.lr_scheduler.OneCycleLR(opt, max_lr, epochs=n_epochs, steps_per_epoch=steps_per_epoch, **kwargs)\n",
    "\n",
    "def make_cbs(metrics, val_dl): \n",
    "    cbs=[]\n",
    "    sd_savename=''\n",
    "    for m in metrics:\n",
    "        metric_cb = callbacks.MetricTrackerCB(m, d_out_val=val_dl.dataset.tfm_df)\n",
    "        cbs.append(metric_cb)\n",
    "        cbs.append(callbacks.SaveModelCBExt(metric_cb, every='best_only', name=f'{sd_savename}_epoch', parent_dir=save_dir))\n",
    "    return cbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jdubindaclub/Desktop/stat486/486_project/deeplearning/transforms.py:101: UserWarning: Since you did not provide df_train, you must run `transform(df, is_train_df=True)` where df is your df_train.\n",
      "This will set self.tr_mean and self.tr_std based on this df. All subsequent calls to transform (ie for val, test, or inf sets) should be with is_train_df=False\n",
      "  warnings.warn(\"Since you did not provide df_train, you must run `transform(df, is_train_df=True)` where df is your df_train.\\n\" + \\\n",
      "/Users/jdubindaclub/Desktop/stat486/486_project/deeplearning/transforms.py:136: UserWarning: A NormalizeTfm has been found in your pipeline without a tr_mean. Be aware that it must come after any transforms that remove rows for it to calculate the training mean and std properly\n",
      "  warnings.warn(\"A NormalizeTfm has been found in your pipeline without a tr_mean. Be aware that it must come after any transforms that \" + \\\n"
     ]
    }
   ],
   "source": [
    "preprocess_pipeline=transforms.Pipeline([\n",
    "    transforms.PrelimPipeline(y_cols), \n",
    "    # transforms.TabPipeline(x_cols, x_cont_cols=cont_cols, x_cat_cols=cat_cols, sk_kwargs={'handle_unknown':'ignore'}), \n",
    "    transforms.TabPipeline(x_cols, x_cont_cols=x_cols, x_cat_cols=None, sk_kwargs={}), \n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing data...\n"
     ]
    }
   ],
   "source": [
    "print('Preprocessing data...')\n",
    "train_df = preprocess_pipeline(train_df_pre.copy(), inference_mode=False, is_train_df=True)\n",
    "val_df = preprocess_pipeline(val_df_pre.copy(), inference_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "unimodal_ds_configs = dict(x_cols=x_cols, pipeline=transforms.Pipeline(), with_label=True)\n",
    "train_ds = datasets.TabDfDataset(train_df, **unimodal_ds_configs, is_train_df=True, y_cols=y_cols)\n",
    "val_ds = datasets.TabDfDataset(val_df, **unimodal_ds_configs, y_cols=y_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, batch_size=train_batch_size,\n",
    "                      num_workers=num_workers, shuffle=train_dl_shuffle, pin_memory=True, drop_last=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=len(val_ds), num_workers=num_workers, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpd = {}\n",
    "tpd['n_iter']=n_iter\n",
    "tpd['max_lr']=max_lr\n",
    "tpd['n_epochs']=n_epochs\n",
    "tpd['pct_start']=pct_start\n",
    "tpd['grad_clip']=grad_clip\n",
    "tpd['num_workers']=num_workers\n",
    "tpd['lams']=lams\n",
    "tpd['cb_metrics']=cb_metrics\n",
    "tpd['train_dl_shuffle']=train_dl_shuffle\n",
    "tpd['train_batch_size']=train_batch_size\n",
    "tpd['val_batch_size']=val_batch_size\n",
    "tpd['model_args']=mod_args\n",
    "tpd['opt_kw']=opt_kw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'emb_szs': [],\n",
       " 'out_sz': 2,\n",
       " 'layers': [3000, 850, 120],\n",
       " 'ps': [0.2, 0.2, 0.05],\n",
       " 'sigma': 0.8}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpd['model_args']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<enumerate object at 0x160b0bd80>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'list' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m fitters \u001b[39m=\u001b[39m []\n\u001b[1;32m      5\u001b[0m start\u001b[39m=\u001b[39mtime\u001b[39m.\u001b[39mtime()\n\u001b[0;32m----> 6\u001b[0m model \u001b[39m=\u001b[39m make_model(\u001b[39mlen\u001b[39;49m(train_dl\u001b[39m.\u001b[39;49mdataset\u001b[39m.\u001b[39;49mx_cols), tpd[\u001b[39m'\u001b[39;49m\u001b[39mmodel_args\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m      7\u001b[0m model\u001b[39m.\u001b[39mto(device);\n\u001b[1;32m      9\u001b[0m criterion \u001b[39m=\u001b[39m loss_fxs\u001b[39m.\u001b[39mEvidentialMSELoss() \n",
      "File \u001b[0;32m~/Desktop/stat486/486_project/deeplearning/util.py:38\u001b[0m, in \u001b[0;36mmake_model\u001b[0;34m(n_cont, model_args)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmake_model\u001b[39m(n_cont, model_args):    \n\u001b[0;32m---> 38\u001b[0m     model \u001b[39m=\u001b[39m FSTabularModel(n_cont\u001b[39m=\u001b[39;49mn_cont, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_args)\n\u001b[1;32m     39\u001b[0m     \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/Desktop/stat486/486_project/deeplearning/model.py:108\u001b[0m, in \u001b[0;36mFSTabularModel.__init__\u001b[0;34m(self, emb_szs, n_cont, out_sz, layers, ps, emb_drop, y_range, use_bn, bn_final, sigma)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, emb_szs:List[\u001b[39mfloat\u001b[39m], n_cont:\u001b[39mint\u001b[39m, out_sz:\u001b[39mint\u001b[39m, layers:List[\u001b[39mint\u001b[39m], ps:List[\u001b[39mfloat\u001b[39m]\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     90\u001b[0m              emb_drop:\u001b[39mfloat\u001b[39m\u001b[39m=\u001b[39m\u001b[39m0.\u001b[39m, y_range\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, use_bn:\u001b[39mbool\u001b[39m\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, bn_final:\u001b[39mbool\u001b[39m\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, sigma:\u001b[39mint\u001b[39m\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m):\n\u001b[1;32m     91\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"This is the feature selection verison of the current TabularModel. If using this model then it is necessary to use \u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[39m    the FSFitter as alterations to the loss function are made. Based on this paper: \u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[39m    https://arxiv.org/pdf/1810.04247.pdf\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[39m        lam (int, optional): Parameter for stochastic gates (see paper). Defaults to 0.01.\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(emb_szs, n_cont, out_sz, layers, ps, emb_drop, y_range, use_bn, bn_final)\n\u001b[1;32m    109\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_selector \u001b[39m=\u001b[39m FeatureSelector(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_emb \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_cont, sigma)\n\u001b[1;32m    110\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msigma \u001b[39m=\u001b[39m sigma\n",
      "File \u001b[0;32m~/Desktop/stat486/486_project/deeplearning/model.py:40\u001b[0m, in \u001b[0;36mTabularModel.__init__\u001b[0;34m(self, emb_szs, n_cont, out_sz, layers, ps, emb_drop, y_range, use_bn, bn_final)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39menumerate\u001b[39m(\u001b[39mzip\u001b[39m(sizes[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m],sizes[\u001b[39m1\u001b[39m:],[\u001b[39m0.\u001b[39m]\u001b[39m+\u001b[39mps,actns)))\n\u001b[1;32m     39\u001b[0m \u001b[39mfor\u001b[39;00m i,(n_in,n_out,dp,act) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mzip\u001b[39m(sizes[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m],sizes[\u001b[39m1\u001b[39m:],[\u001b[39m0.\u001b[39m]\u001b[39m+\u001b[39mps,actns)):\n\u001b[0;32m---> 40\u001b[0m     layers \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m flayers\u001b[39m.\u001b[39;49mbn_drop_lin(n_in, n_out, bn\u001b[39m=\u001b[39;49muse_bn \u001b[39mand\u001b[39;49;00m i\u001b[39m!=\u001b[39;49m\u001b[39m0\u001b[39;49m, p\u001b[39m=\u001b[39;49mdp, actn\u001b[39m=\u001b[39;49mact)\n\u001b[1;32m     41\u001b[0m \u001b[39mif\u001b[39;00m bn_final: layers\u001b[39m.\u001b[39mappend(nn\u001b[39m.\u001b[39mBatchNorm1d(sizes[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]))\n\u001b[1;32m     42\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSequential(\u001b[39m*\u001b[39mlayers)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/fastai/layers.py:47\u001b[0m, in \u001b[0;36mbn_drop_lin\u001b[0;34m(n_in, n_out, bn, p, actn)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[39m\"\u001b[39m\u001b[39mSequence of batchnorm (if `bn`), dropout (with `p`) and linear (`n_in`,`n_out`) layers followed by `actn`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     46\u001b[0m layers \u001b[39m=\u001b[39m [nn\u001b[39m.\u001b[39mBatchNorm1d(n_in)] \u001b[39mif\u001b[39;00m bn \u001b[39melse\u001b[39;00m []\n\u001b[0;32m---> 47\u001b[0m \u001b[39mif\u001b[39;00m p \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m: layers\u001b[39m.\u001b[39mappend(nn\u001b[39m.\u001b[39;49mDropout(p))\n\u001b[1;32m     48\u001b[0m layers\u001b[39m.\u001b[39mappend(nn\u001b[39m.\u001b[39mLinear(n_in, n_out))\n\u001b[1;32m     49\u001b[0m \u001b[39mif\u001b[39;00m actn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m: layers\u001b[39m.\u001b[39mappend(actn)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/dropout.py:15\u001b[0m, in \u001b[0;36m_DropoutNd.__init__\u001b[0;34m(self, p, inplace)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, p: \u001b[39mfloat\u001b[39m \u001b[39m=\u001b[39m \u001b[39m0.5\u001b[39m, inplace: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[0;32m---> 15\u001b[0m     \u001b[39mif\u001b[39;00m p \u001b[39m<\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39mor\u001b[39;00m p \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m     16\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdropout probability has to be between 0 and 1, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     17\u001b[0m                          \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(p))\n\u001b[1;32m     18\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mp \u001b[39m=\u001b[39m p\n",
      "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'list' and 'int'"
     ]
    }
   ],
   "source": [
    "steps_per_epoch = len(train_dl)\n",
    "lr_kwargs['steps_per_epoch']=steps_per_epoch\n",
    "\n",
    "fitters = []\n",
    "start=time.time()\n",
    "model = make_model(len(train_dl.dataset.x_cols), tpd['model_args'])\n",
    "model.to(device);\n",
    "\n",
    "criterion = loss_fxs.EvidentialMSELoss() \n",
    "criterion = loss_fxs.CombinedInternalLosses(model.feature_selector, lambdas=torch.Tensor([lams]), supervised_loss=criterion)\n",
    "\n",
    "fitters.append(fitting.EvidentialFitter(model, train_dl, val_dl, criterion, \n",
    "                                        grad_clip, device, lr_sched_factory, lr_kwargs=lr_kwargs, \n",
    "                                        callbacks=make_cbs(cb_metrics, val_dl), opt_kwargs=opt_kw,\n",
    "                                        quiet=True))\n",
    "predictors = [f.fit(n_epochs=n_epochs) for f in fitters]\n",
    "end=time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{(end-start)/60:.2f} mins\")\n",
    "tpd['total_train_time']=f\"{(end-start)/60:.2f} mins\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(save_dir,'tpd.pkl'), 'wb') as f:\n",
    "    pickle.dump(tpd, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dr. Heaton Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"smoking_drinking_hyperparams.csv\")\n",
    "df['DRK_YN'] = df['DRK_YN'].map({'Y': 1, 'N': 0})\n",
    "\n",
    "#dummies for sex\n",
    "df = pd.concat([df,pd.get_dummies(df['sex'],prefix=\"sex\",dtype=int)],axis=1)\n",
    "df.drop('sex', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to pytorch tensors\n",
    "x_columns = df.columns.drop(['DRK_YN'])\n",
    "x = torch.tensor(df[x_columns].values, dtype=torch.float32, device=device)\n",
    "y = torch.tensor(df['DRK_YN'].values, dtype=torch.float32, device=device).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "patience = 10\n",
    "\n",
    "fold = 0\n",
    "for train_idx, test_idx in kf.split(x):\n",
    "    fold += 1\n",
    "    print(f\"Fold #{fold}\")\n",
    "\n",
    "    x_train, x_test = x[train_idx], x[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    # PyTorch DataLoader\n",
    "    train_dataset = TensorDataset(x_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=108, shuffle=True)\n",
    "\n",
    "    # Create the model and optimizer\n",
    "    model = make_model(len(train_loader.dataset.tensors[0][0]), mod_args)\n",
    "\n",
    "        # Create the model and optimizer\n",
    "    # model = nn.Sequential(\n",
    "    #     nn.Linear(x.shape[1], 20),\n",
    "    #     nn.BatchNorm1d(20),  # BatchNorm layer\n",
    "    #     nn.ReLU(),\n",
    "    #     nn.Linear(20, 10),\n",
    "    #     nn.BatchNorm1d(10),  # BatchNorm layer\n",
    "    #     nn.ReLU(),\n",
    "    #     nn.Linear(10, 1),\n",
    "    #     nn.Sigmoid()  # Sigmoid activation for binary classification\n",
    "    # )\n",
    "    # model = torch.compile(model,backend=\"aot_eager\").to(device)\n",
    "\n",
    "    # Early Stopping variables\n",
    "    best_loss = float('inf')\n",
    "    early_stopping_counter = 0\n",
    "\n",
    "    # Training loop\n",
    "    EPOCHS = 5\n",
    "    epoch = 0\n",
    "    done = False\n",
    "    es = EarlyStopping()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    loss_fn = loss_fxs.EvidentialMSELoss(evidence_activation_fx=loss_fxs.relu_evidence, device=device).edl_mse_loss()\n",
    "    # loss_fn = edl_mse_loss(device=device)\n",
    "\n",
    "    while not done and epoch < EPOCHS:\n",
    "        epoch += 1\n",
    "        model.train()\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x_batch)\n",
    "            # Ensure y_batch is the correct shape, e.g., (batch_size, 1) for BCEWithLogitsLoss\n",
    "            loss = loss_fn(output, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        total=0\n",
    "        correct=0\n",
    "        with torch.no_grad():\n",
    "            val_output = model(x_test)\n",
    "            # Ensure y_test is the correct shape, similar to y_batch\n",
    "            val_loss = loss_fn(val_output, y_test)\n",
    "\n",
    "        if es(model, val_loss):\n",
    "            done = True\n",
    "\n",
    "    print(f\"Epoch {epoch}/{EPOCHS}, Validation Loss: {val_loss.item()}, {es.status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_heaton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"smoking_drinking_hyperparams.csv\")\n",
    "\n",
    "X = df.drop(columns=[\"DRK_YN\"])\n",
    "y = df['DRK_YN'].map({'Y': 1, 'N': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_cols = [\n",
    "    \"hear_left\", \n",
    "    \"hear_right\", \n",
    "    \"urine_protein\", \n",
    "    \"SMK_stat_type_cd\",\n",
    "    ]\n",
    "\n",
    "nominal_cols = [\n",
    "    \"sex\",\n",
    "    \"SMK_stat_type_cd\",\n",
    "    ]\n",
    "\n",
    "numeric_cols = [\n",
    "    \"age\",\n",
    "    \"height\",\n",
    "    \"weight\",\n",
    "    \"waistline\",\n",
    "    \"sight_left\",\n",
    "    \"sight_right\",\n",
    "    \"SBP\",\n",
    "    \"DBP\",\n",
    "    \"BLDS\",\n",
    "    \"tot_chole\",\n",
    "    \"HDL_chole\",\n",
    "    \"LDL_chole\",\n",
    "    \"triglyceride\",\n",
    "    \"hemoglobin\",\n",
    "    \"serum_creatinine\",\n",
    "    \"SGOT_AST\",\n",
    "    \"SGOT_ALT\",\n",
    "    \"gamma_GTP\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change preprocessor to FIRST encode ordinal/nominal columns, THEN standardize all columns using zscore instead of standard scaler\n",
    "\n",
    "ordinal_transformer = Pipeline(steps=[\n",
    "    ('ordinal', OrdinalEncoder())\n",
    "])\n",
    "\n",
    "nominal_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('ord', ordinal_transformer, ordinal_cols),\n",
    "        ('nom', nominal_transformer, nominal_cols)\n",
    "    ],\n",
    "    remainder='passthrough'  #stops pipeline from dropping numeric columns\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('zscore_scaler', ZScoreScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['SMK_stat_type_cd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transformed = pipeline.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_heaton = pd.read_csv(\n",
    "    \"https://data.heatonresearch.com/data/t81-558/jh-simple-dataset.csv\",\n",
    "    na_values=['NA','?'])\n",
    "df_heaton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_heaton = pd.concat([df_heaton,pd.get_dummies(df_heaton['job'],prefix=\"job\",dtype=int)],axis=1)\n",
    "df_heaton.drop('job', axis=1, inplace=True)\n",
    "df_heaton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
