{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_evidence(y):\n",
    "    return torch.nn.functional.relu(y)\n",
    "\n",
    "def softplus_evidence(y):\n",
    "    return torch.nn.functional.softplus(y)\n",
    "\n",
    "def kl_divergence(alpha, device='cpu'):\n",
    "    num_classes=alpha.shape[-1]\n",
    "    ones = torch.ones([1, num_classes], dtype=torch.float32, device=device)\n",
    "    sum_alpha = torch.sum(alpha, dim=1, keepdim=True)\n",
    "    first_term = (\n",
    "        torch.lgamma(sum_alpha)\n",
    "        - torch.lgamma(alpha).sum(dim=1, keepdim=True)\n",
    "        + torch.lgamma(ones).sum(dim=1, keepdim=True)\n",
    "        - torch.lgamma(ones.sum(dim=1, keepdim=True))\n",
    "    )\n",
    "    second_term = (\n",
    "        (alpha - ones)\n",
    "        .mul(torch.digamma(alpha) - torch.digamma(sum_alpha))\n",
    "        .sum(dim=1, keepdim=True)\n",
    "    )\n",
    "    kl = first_term + second_term\n",
    "    return kl\n",
    "\n",
    "def loglikelihood_loss(y, alpha, device='cpu'):\n",
    "    y = y#.to(device)\n",
    "    alpha = alpha#.to(device)\n",
    "    S = torch.sum(alpha, dim=1, keepdim=True)\n",
    "    loglikelihood_err = torch.sum((y - (alpha / S)) ** 2, dim=1, keepdim=True)\n",
    "    loglikelihood_var = torch.sum(\n",
    "        alpha * (S - alpha) / (S * S * (S + 1)), dim=1, keepdim=True\n",
    "    )\n",
    "    loglikelihood = loglikelihood_err + loglikelihood_var\n",
    "    return loglikelihood\n",
    "\n",
    "def edl_loss(func, y, alpha, epoch_num, annealing_step, device=None):\n",
    "    y = y#.to(device)\n",
    "    alpha = alpha#.to(device)\n",
    "    S = torch.sum(alpha, dim=1, keepdim=True)\n",
    "\n",
    "    A = torch.sum(y * (func(S) - func(alpha)), dim=1, keepdim=True)\n",
    "\n",
    "    annealing_coef = torch.min(\n",
    "        torch.tensor(1.0, dtype=torch.float32),\n",
    "        torch.tensor(epoch_num / annealing_step, dtype=torch.float32),\n",
    "    )\n",
    "\n",
    "    kl_alpha = (alpha - 1) * (1 - y) + 1\n",
    "    kl_div = annealing_coef * kl_divergence(kl_alpha, device=device)\n",
    "    return A + kl_div\n",
    "\n",
    "def mse_loss(y, alpha, epoch_num, annealing_step=10, device='cpu'):\n",
    "    y = y#.to(device)\n",
    "    alpha = alpha#.to(device)\n",
    "    loglikelihood = loglikelihood_loss(y, alpha, device=device)\n",
    "\n",
    "    annealing_coef = torch.min(\n",
    "        torch.tensor(1.0, dtype=torch.float32),\n",
    "        torch.tensor(epoch_num / annealing_step, dtype=torch.float32),\n",
    "    )\n",
    "\n",
    "    kl_alpha = (alpha - 1) * (1 - y) + 1\n",
    "    kl_div = annealing_coef * kl_divergence(kl_alpha, device=device)\n",
    "    return loglikelihood + kl_div\n",
    "\n",
    "def edl_log_loss(output, target, epoch_num, annealing_step, device=None):\n",
    "    evidence = relu_evidence(output)\n",
    "    alpha = evidence + 1\n",
    "    loss = torch.mean(\n",
    "        edl_loss(\n",
    "            torch.log, target, alpha, epoch_num, annealing_step, device\n",
    "        )\n",
    "    )\n",
    "    return loss\n",
    "\n",
    "class PenalizedTanh(torch.nn.Module):\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x[x>0]=nn.Tanh()(x[x>0])\n",
    "        x[x<=0]=0.25*nn.Tanh()(x[x<=0])\n",
    "        return x\n",
    "\n",
    "class EvidentialMSELoss():\n",
    "    \"\"\"\n",
    "    I made this a class so that you can give it certain default parameters (like evidence_activation_fx) and can then pass it to a fitter\n",
    "    Because of the '__call__' method, you can still just call the EdlMSELoss object like you would a normal function\n",
    "    \"\"\"\n",
    "    def __init__(self, evidence_activation_fx=torch.nn.functional.softplus, annealing_step=10, device='cpu'):\n",
    "        self.evidence_activation_fx, self.annealing_step, self.device = evidence_activation_fx, annealing_step, device\n",
    "        # todo take out device from loss fxs now that it is in class init\n",
    "    def _one_hot_embedding(self, labels, num_classes):\n",
    "        y = torch.eye(num_classes).float().to(self.device) \n",
    "        return y[labels]\n",
    "\n",
    "    def edl_mse_loss(self, output, target, epoch_num, device='cpu', losswts=None, reduction='mean'):\n",
    "        y_onehot = self._one_hot_embedding(target, num_classes=output.shape[-1])\n",
    "        evidence = self.evidence_activation_fx(output)\n",
    "        alpha = evidence + 1\n",
    "        loss = mse_loss(y_onehot, alpha, epoch_num, self.annealing_step, device=device)\n",
    "        if losswts is not None: loss*=losswts.view(-1,1)\n",
    "        if reduction=='mean': loss = torch.mean(loss)\n",
    "        return loss.flatten()\n",
    "    \n",
    "    def edl_digamma_loss(self, output, target, epoch_num, device='cpu', losswts=None, reduction='mean'):\n",
    "        y_onehot = self._one_hot_embedding(target, num_classes=output.shape[-1])\n",
    "        evidence = self.evidence_activation_fx(output)\n",
    "        alpha = evidence + 1\n",
    "        loss = edl_loss(torch.digamma, y_onehot, alpha, epoch_num, self.annealing_step, device)\n",
    "        if losswts is not None: loss=loss.view(-1,output.shape[-1])*losswts.view(-1,1)\n",
    "        if reduction=='mean': loss = torch.mean(loss)\n",
    "        return loss.flatten()\n",
    "    \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.edl_mse_loss(*args, **kwargs)\n",
    "        # return self.edl_digamma_loss(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EvidentialMSELoss(evidence_activation_fx=torch.nn.functional.relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
